In my discussions with data teams from many big tech companies, I've noticed that a lot of a data engineer's work is batch processing to serve data for analytics, and in some cases, for training machine learning models. With batch transformations, you manipulate discrete chunks of data on a fixed schedule, like daily, hourly, every 15 minutes, or less, to support ongoing reporting, analytics, and machine learning use cases. Let’s go through some common batch transformation patterns.
Suppose you created a model for your data, it could be based on a Kimball star schema, data vault, or some other data modeling approach. Now you’re ingesting data into your system, and you need to apply transformations to restructure the source data into the expected form. You have a few options here.

You could follow a traditional ETL approach and rely on an external transformation tool to extract and transform the data based on the data model you created, before loading the transformed data into a target system, such as a data warehouse. Or, you can follow an ELT approach where you extract raw data from a source system and import it directly into a data warehouse, then you clean and transform the data in the warehouse itself, relying on the storage and computing capabilities of the warehouse. Or, you can take sort of a hybrid approach called EtLT, where the little “t” refers to the simple transformations you apply to clean the data, like deduplicating the data, before you load the cleaned data into a data warehouse. Then the big “T” refers to the transformations you apply inside the data warehouse to restructure the data based on the model that you defined.

In fact, you already experimented with both the ETL and ELT approaches in previous labs. Back in course 1, you implemented an ETL pipeline with AWS Glue ETL as the external transformation tool to transform the data before loading it into S3. AWS Glue ETL is based on a distributed processing framework called Spark, and you can use it to perform more complex transformations on larger datasets. We’ll cover Spark in more detail later this week. Then in the first week of this course, you implemented an ELT pipeline, using dbt to transform the data within a database itself. Note that dbt is not an execution tool like Spark, meaning it does not come with computing resources, it is instead a SQL-tool that you can use to facilitate the transformation tasks within a database or data warehouse by relying on the computing resources of the storage system.

In addition to transforming your data into a target schema, you might need to apply transformations to clean and normalize your data. For example, the source data you extracted might have missing values, duplicate entries, outliers or other inconsistencies. This process of taking messy, malformed data and turning it into clean data is called data wrangling. 

You could write your own code to perform data wrangling, like you did last week to turn raw data into a useful form for training a machine learning algorithm. But I highly recommend you use a data wrangling tool to avoid undifferentiated heavy lifting. There are many third-party data wrangling tools available to you, and many major cloud providers typically offer their own version of these tools. For example, AWS offers AWS Glue DataBrew as a visual data preparation service for cleaning, standardizing, and transforming data. 

After you store your transformed data in the data pipeline, you might need to periodically or continuously update the data to make sure that it is in-sync with the data in the source system. You can apply a simple approach known as “truncate and reload”, where you delete all records from your table and then reload the data from the data source, rerunning any transformations needed to get the data into your target system. This approach works when you have a small dataset and only need to update the data in your target system once in a while. However, if your dataset is large, this approach can become very resource intensive. 

In this case, you might want to adopt a CDC or Change Data Capture approach, where you first identify the changes made in the source system, and then update the tables in your target system based only on those changes. For example, you can check the “last updated” column in a relational source database, or you can check the database’s transactional logs. Each row in the log can be labeled with an “I” if the row was inserted, a “U” if the row was updated, or a “D” if the row was deleted.

When handling updated rows, you can apply an “insert-only” pattern or “upsert/merge” pattern to update your target system. With an insert-only pattern, you insert new records without changing or deleting old records, and you add additional information to the new record to distinguish it from the old one. With an “upsert/merge” pattern, you take a set of source records and look for matches against your target table by using a primary key or another logical condition. When a key match occurs, you update the target record by replacing it with the new record. When no match exists, you insert the new record. 

When handling deleted rows, you can adopt a hard or soft delete pattern. With a hard delete, you permanently remove a record from your target system. Whereas with a soft delete, you mark the record as “deleted.” You might use hard deletes to remove data for performance reasons – say, a table is too big – or if there’s a legal or compliance reason to do so. And you can use soft deletes when you don’t want to permanently delete a record but you also want to filter it out of query results. You can also delete a record in an insert-only manner where you insert a new record with a deleted flag without modifying the previous version of the record. 

So, single-row inserts are commonly performed on row-oriented databases. However, a problem I see often is that some data engineers try to perform single-row inserts into an OLAP column-oriented database. This is an antipattern that could put a massive load on the OLAP system. It also causes data to be written in many separate files, which is extremely inefficient for subsequent reads. So instead, I recommend loading data in a periodic micro-batch or batch fashion. When you insert data in bulk, the data can be organized more efficiently into row groups and better compressed. And if the OLAP system is distributed, you can leverage the distributed parallel processing capability instead of loading records one by one. 

With the explosion of data in the early 2000’s, traditional monolithic databases and data warehouses of the 1990s were not able to handle massive amounts of data in a cost-effective, scalable, available, and reliable way. At the same time, commodity hardware—such as servers, RAM, disks, and flash drives—also became cheap and ubiquitous. 

During this time, several innovations led to the large-scale distributed computing and storage on massive computing clusters that you see today. In 2003, Google published a paper on the Google File System or GFS, which provided a fault-tolerant and distributed file system across many clusters of commodity hardware servers. Shortly after that, in 2004, Google published a paper on MapReduce, a new parallel programming paradigm for large-scale processing of data distributed over GFS. Google’s publications constituted a “big bang” for data technologies and the cultural roots of data engineering. 

The Google papers inspired engineers at Yahoo to develop the open source framework Apache Hadoop in 2006. Google’s GFS paper provided a blueprint for the Hadoop Distributed File System, or HDFS, and MapReduce became part of the framework. Although Hadoop is not considered a bleeding edge technology today, I think it’s important for you to understand the concepts behind Hadoop, because MapReduce still influences many distributed systems that data engineers use today, and HDFS is still a key ingredient of many current big data engines, such as Amazon EMR and Spark. 

The Hadoop Distributed File System is similar to object storage but with a key difference: Hadoop combines compute and storage on the same nodes, whereas object storage typically has limited compute support for internal processing.

Hadoop breaks large files into blocks, each block holding a chunk of data less than a few hundred megabytes in size. The filesystem is managed by what’s called the NameNode, which maintains directories, file metadata, and a detailed catalog describing the location of the file blocks in the cluster. In a typical configuration, you would replicate each block of data to three nodes called DataNodes. This increases both the durability and availability of data. If a disk or node fails and the replication factor for some file blocks fall below 3, the NameNode will instruct other DataNodes to replicate these file blocks so that they again reach the correct replication factor. By combining compute resources with storage nodes, Hadoop allows in-place data processing. This was originally achieved using the MapReduce programming model.

In the MapReduce programming model, you send computation code to the nodes that contain the data, favoring locality rather than bringing data to your application. The computation code consists of a collection of map tasks that read individual data blocks and produce a set of key-value pairs. These map tasks are followed by a shuffle that redistributes the results across the cluster so that values with the same key are collected together in the same node, and finally a reduce step that aggregates data on each node. 

For example, suppose that you wanted to run this SQL query that SELECTs the user id, and count star, meaning the count of all records, FROM the user events table, and you’ll GROUP BY the user id. So the result will be all the user ids along with the number of records associated with that user id.

With HDFS, the data in the user events table is broken down into data blocks and spread across many nodes. Let’s zoom in on one DataNode that contains these 3 data blocks. The MapReduce job generates one map task per block. Each map task essentially runs the query on their respective block, and generates key-value pairs where the key represents a user id that appears in the block and the value is the corresponding count of records associated with that user id within that block. So for example, in this first block, there are 6 records associated with user2 and 10 records associated with user1, and so on. While the table might contain petabytes of data, each block might only contain hundreds of megabytes, so it’s much faster to run the query on a single block than it is on the full table. You then redistribute the results by key, which is the user id in this example, so that each key ends up on one and only one node. So here, all the user1 key-value pairs end up in one DataNode, and the user2 key-value pairs end up in another DataNode. This is the shuffle step, which is often executed using a hashing algorithm on keys. Once the map results have been shuffled, you sum the results for each key. The key, along with its computed total count, can be written to the local disk on the node where they are computed. Then finally, you collect the results stored across nodes to view the full query results. 

This model is extremely powerful but it has some shortcomings. It utilizes numerous short-lived MapReduce tasks that read data from disk and write intermediate computations to disk. In particular, no intermediate state is preserved in memory, and all data is transferred between tasks by storing it on disk or pushing it over the network. This simplifies state and workflow management and minimizes memory consumption, but it can also drive high-disk bandwidth utilization and increase processing time. So engineers developed other data processing frameworks that still include some elements of map, shuffle, and reduce, but relax the constraints of MapReduce to allow for in-memory caching. And since RAM is much faster than SSD and HDDs in transfer speed and seek time, persisting even a tiny amount of chosen data in memory can dramatically speed up specific data processing tasks. For example, Spark, which we’ll discuss in the next video, was designed around in-memory processing. WIth Spark, you treat data as a distributed set that resides in memory, and treat disk as a second-class data-storage layer for processing, which you only use if your data overflows from the available memory.

Unlike Hadoop, which integrates compute and persistent storage via the Hadoop Distributed File System, Spark is just a computing engine designed for processing distributed large datasets. With Spark, you can perform parallel computations and retain intermediate results in memory, which limits disk I/O interactions, enabling significantly faster computation than Hadoop MapReduce. You can use Spark locally, in data centers, or in the cloud. You can  load data from and store the final results on separate persistent storage systems such as relational or key-value databases, object storage, or even Hadoop File System.

Spark provides a unified platform that allows you to run different types of analytical workloads as self-contained Spark applications on one processing engine. For example, you can use Spark to perform SQL queries to load data, train and test Machine learning algorithms, and apply streaming transformations on your data over the same computation engine. You can write your workloads in Python, Java, R and Scala using Spark core APIs. Spark also offers built-in libraries such as Spark SQL for writing SQL queries, MLlib for machine learning applications, Spark Structured Streaming for interacting with real-time data, and GraphX for graph processing. Beyond these standard libraries, you can also use external third-party libraries published and maintained by the open source communities. These include connectors that allow you to connect to a variety of external data sources and storage systems, monitor performance and more.

Let’s zoom in on the underlying components of a Spark application and see how they work together to process your code. A Spark application consists of a cluster of nodes: 
•	It has a driver node, which is the central controller of a Spark application;
•	a cluster manager node, which communicates with the driver to allocate computational and memory resources across a cluster, and manage these resources;
•	and a set of worker nodes, where each node contains a Spark executor that executes tasks assigned to them by the driver. Spark applies a partitioning scheme to break up the data into partitions when loading it from disk, and allocates to each spark executor the partitions that are closest to it in the network. So, each executor’s CPU core gets a partition of data to work on.

When you write a Spark application, you start by instantiating a Spark session object that represents a single unified entry point to all Spark’s functionality. Through this entry, you can define DataFrames, read data from sources, and perform SQL queries. The driver node translates the instructions you wrote, which could be in Python, Scala, or another language, into Spark jobs, which will be executed one by one based on the job’s priority. To do this, the driver transforms each job into a sequence of stages, and represents these stages as a DAG. For example, this job here has 3 stages and is represented with this DAG. So each DAG is sort of like the execution plan for the corresponding job. Each stage is further broken down into tasks written in Spark code that can run in parallel. Here, stage 1 has 4 tasks that can run in parallel, and stages 2 and 3 each have 3 tasks that can also run in parallel. You’ll run stages that have shared dependencies serially, and run those without dependencies in parallel. For example, in this DAG, stages 2 and 3 depend on the results of stage 1, so they can’t start until stage 1 is done. But stages 2 and 3 don’t have any shared dependencies and can run in parallel. So to execute this job, Spark will start with stage 1 and run all 4  tasks in parallel. Once these tasks are done, stage 2 and 3 will both start, and the 3 tasks in each stage will run in parallel.

Returning to our Spark application, once the DAG execution plan is developed, the driver communicates with the cluster manager to request computational and memory resources for the executors. Each task is assigned to a single core within an executor, and each executor works on a single data partition. The executor executes the task and communicates the computation results back to the driver node. And finally, the driver node aggregates these computations and returns the results back to you.


Spark dataframes are actually built on top of a low-level data structure called Resilient Distributed Dataset or RDD, which represents the actual partitioned collection of records that can be operated on in parallel. To work directly with RDDs, you would need to manually define and optimize all the operations you want to perform on your data. But with Spark Dataframes, you can interact with the data using simpler and more expressive high-level operations, such as filtering, selecting, counting, aggregating and grouping, and Spark will compile these operations down to the RDD level behind the scenes.

Spark DataFramaes and the underlying RDDs are both considered to be immutable data structures, which is what makes them resilient, in other words, fault tolerant. 

You can classify the operations on distributed data into two types: transformations and actions. Transformations, such as filtering, selecting, joining and grouping, create new DataFrames from existing ones without modifying the original data. This is why DataFrames and their underlying RDDs are considered immutable. Actions, such as count, show, and save, trigger the execution of these transformations. In fact, all Spark transformations are evaluated lazily, meaning that they are not executed immediately. Instead, they are recorded as a lineage and only executed when an action is invoked. This lazy evaluation allows Spark to optimize the execution plan by rearranging transformation operations for efficiency. Moreover, the lineage and immutability properties ensure fault-tolerance, because these properties allow you to reproduce an original state in the event of failures.

For this demo, I’ll use this pip install command to install PySpark locally. In the lab, you will use Amazon EMR to deploy a cluster for running Spark. And I’ve included links in the additional resource section at the end of this week to show you how you can download the full version of Spark.
<lab option 1>>

For this demo, I’ll use this pip install command to install PySpark locally. In the lab, you will be provided with a Spark cluster running inside a Docker container, but there are other ways you can run this processing system, for example, you can use Amazon EMR to deploy a cluster for running Spark. And I’ve included links in the additional resource section at the end of this week to show you how you can download the full version of Spark.
Next, let’s install another library called findspark, which will automatically add pyspark to the system’s path during runtime, so that your system knows where to find spark. I’ll import both the pyspark and findspark packages, and then I’ll initialize findspark.

Before you can create a Spark DataFrame, you need to create a SparkSession, which works as the entry point for working with Spark DataFrames. So, from pyspark.sql, let’s import the SparkSession class. Then I’ll create a spark session with the name “example” by calling this “getOrCreate” method. This method will get an existing SparkSession with the name “example” if it exists, or it will create a new one. Here are the details of this session, you can see the version number of this SparkSession and that its name is indeed “example”.

Now let’s create a DataFrame. You can do this manually, or you can import data from external data sources. Let’s say you want to manually create a DataFrame that holds the data from this OrderDetails table. I’ll create a DataFrame called orders df by specifying the SparkSession object that I created from before, and then calling the “createDataFrame” method on it. This method expects two arguments. The first argument is the data, which I’ll provide as a list of tuples, each consisting of the data from one row. The second argument is the schema, so I’ll specify the name and type of each column. You can use the “show” method to view the rows of this DataFrame.

Let’s say instead, you want to create a DataFrame by reading data from a CSV file that contains transaction data of an online retail store. Let’s create another DataFrame called transactions df that uses the same SparkSession object as before, but this time, it calls the read.csv method, specifying the name of the CSV file, and indicating that the data contains a header. You can view the first 5 rows of this data by calling the “show” method with n equals 5.

Now, let’s say you only care about some of the columns in this DataFrame. You can first use the columns command to view all the column names, then you can select only the columns you want by using the “select” method. So here I’ll select only the Price, Quantity and Country columns, and then I’ll use the “show” method to view the first 5 rows of these three columns. If you want to quickly explore the content of these columns, you can use the “describe” method to compute basic summary statistics for each column, including the value count, mean, standard deviation, min and max of each column like you see here. 

Next, let’s take a look at how you can manipulate DataFrames, such as adding, updating, renaming or removing a column from an existing DataFrame. Say you want to create a new column that represents the amount paid for each product. You can call the “withColumn” method, then specify the name of this new column – which I’ll call “Amount”, and the values for this new column – which I’ll get by multiplying the price and quantity columns from the existing DataFrame. Spark checks if the “Amount” column already exists in the DataFrame, if it does, then it’ll replace this column with the new values, and if this column doesn’t exist, then it will create a new column with these values. Remember that Spark DataFrames are immutable, so this method actually returns a new DataFrame, but I can assign this new DataFrame back to the same transactions df variable. You can verify that this new column was indeed added by calling the “show” method.  Next, let’s rename the “Invoice” column to “ID”. For that, I’ll call the “withColumnRenamed” method, and specify that the name of the existing column is “Invoice”, and that the new column name should be “ID”. And finally let’s remove the “Description” column. I’ll call the “drop” method and specify the name of the column that I want to remove.

In terms of cleaning your data in a DataFrame, you can easily remove the rows that contain null values by calling the “dropna” method. You can also call the “filter” method to remove rows based on boolean conditions. For example, let’s say that the “quantity” column contains some negative values, and you only want to consider the rows that contain positive values. You can call the “filter” method and provide it with the condition where the “quantity” column must be greater than 0. That way,  you’ll only get the rows that don’t contain null values and contain positive quantity values.

Next, let’s take a look at some aggregation operations on DataFrames. For example, suppose you want to find the total amount spent on each order. First I’ll use the “groupby” method to group the rows by the order ID. Next I’ll “sum” up the “Amount” column to get the total amount for each order ID. I’ll call the “show” method to view the results. As another example, let’s count the total number of rows for each country and order the count in descending order. Again I’ll call the “groupby” method to group the rows by country, and then I’ll call the “count” method. To sort the results, I’ll call the “orderBy” method to order by the count, and specify false for ascending so that the results are sorted in descending order.

While Spark provides lots of built-in functions that you can find in the documentation, it also supports user-defined functions, or UDFs. For example, let’s say you want to convert all the country names to uppercase. You can define a function called “to Upper” that takes in a string and returns the string in uppercase. Then, you need to wrap this function in a udf object. To do so, let’s import the udf class from pyspark sql functions. Then I’ll instantiate a udf object called “udf to upper”, specify the function that I just defined, along with its return type. Now I’ll select the ID column and the country column, and apply the “udf to upper” object to the country column. You can see that all entries in the country column have now been transformed to uppercase strings. 

Alternatively, instead of wrapping the function you defined inside a udf object, you can add this @udf decorator with the returned type inside the brackets. Then you can directly apply the to Upper function to the country column. 

Again, remember that Spark DataFrames are immutable. So, with the UDF, you actually created a new DataFrame that has the country column in uppercase. Let’s replace the country column in the transactions dataframe with a column containing the values in uppercase. I’ll call the “withColumn” method and specify that I want to replace the values in the “country” column with their uppercase values. And here’s the new transactions dataframe. 

Now that you’ve seen how Python UDFs work, you should be careful when working with these functions. This is because Spark is native in Scala, and each executor in the worker node is a Java Virtual Machine or JVM process that hosts a partition of data. When you use Python UDFs, Spark starts a separate python process inside the worker node, and requires data transfer from the JVM. For the data to be processed by python, Spark serializes it to a format that Python can understand. The Python process then executes the function row by row on that data, and finally returns the results of the row operations to the JVM. This process is not efficient since the JVM and python processes will compete for memory resources, and it’s expensive to serialize and deserialize the data. For better performance, you should consider rewriting Python UDFs in Scala or Java because these functions will run directly within the JVM. And the good news is after you register these UDFs in Spark, you can still use them in Python.

Here we have the same jupyter notebook that you saw in the previous video, and here’s the transactions DataFrame we ended up with. To issue SQL queries to interact with this data, you need to create a temporary view from the dataframe. A temporary view is a virtual table that doesn’t actually hold the data. It persists as long as the spark session is running, and provides an interface for you to work with the table data using SQL code. So here I’ll call the “createOrReplaceTempview” method on the transactions dataframe to create a temporary view called orders.

Now you can issue SQL queries on this temporary orders table! Using the spark session object, you can call the SQL method and specify the SQL query as a string inside the brackets. This method will return a dataframe representing the results of the query. For example, let’s say you want to find the total amount spent on each order. I’ll SELECT the ID {pause} and the SUM of the amount column, which I’ll rename as total {pause}FROM the orders table, {pause}and GROUP BY the ID. And then I’ll order the results in descending order. Here’s the returned dataframe. You can see the total amounts spent in descending order.

You can also define your own function and use it inside the SQL query. When working with DataFrames, you saw that you need to either wrap your function inside a udf object or use the udf decorator. With SQL queries, you need to register your function. As an example, let’s write a function called “to lower” that transforms a word into lowercase letters. To register this function, I’ll call the register method using the spark session object. Then for the first argument, I’ll specify a name for the function, which I’m calling “udf to lower”, and then I’ll pass in the function itself as the second argument. Now using the name of this function, you can use it inside the SQL query. For example, you can SELECT the DISTINCT countries FROM the orders table, and apply this “udf to lower” function on the country column. 

You can also create more than one view, meaning more than one virtual table, and join them in a SQL query. For example, let’s first create another DataFrame called “product category df” that contains the code and the category of three products. Then using this DataFrame, I’ll create another temporary view called items. Then in this SQL query I’ll join the orders tables from before with this new items table to find the average amount for each category. So I’ll SELECT the category {pause} and the AVERAGE amount {pause} FROM the items table LEFT JOINed with the orders table based on the itemID from the items table and the StockCode from the orders table. And then I’ll GROUP BY the category so I can find the average order amount per category.

If you remember our earlier lesson about Amazon Redshift, you learned how it uses massively parallel processing to tackle big data analytics. EMR works in a similar way, in that there is a cluster with multiple nodes and each node does a portion of the work.

When you submit a job using EMR, it gets run across these nodes working in parallel, and each node processes a part of the data. Because of this parallelization, the job is completed much quicker than what could be achieved by a single machine.

The size of your cluster can impact how quickly your jobs run, and with EMR a cluster is elastic, meaning that it can scale up or down as needed.

Once your job completes, the results are stored in your desired destination. This could be in Amazon S3, HDFS, or another data store option. You can then analyze the results or feed them into another application or workflow for further processing.

EMR also supports numerous popular big data frameworks including Apache Spark, Hadoop, Hive, Presto, Flink, HBase, and many more tools and frameworks that enable data analysis tasks.

EMR provides a managed environment that simplifies the setup and scaling of these frameworks, and integrates natively with other AWS services. So, with EMR, you can focus more on your data workflows rather than the underlying infrastructure.

For example, if you want to analyze data stored in S3 with Hadoop you can do that with the integration between S3 and the Amazon EMR file system. This allows you to decouple compute and storage, and analyze large amounts of data that may not be able to fit on local storage.

The way this works is that when you launch your cluster, EMR streams the data from S3 to each instance in your cluster and begins processing it.

Another advantage of storing your data in S3 is you can use multiple EMR clusters to process the same data set in different ways.

EMR can also integrate with other AWS data sources like Amazon DynamoDB, Amazon RDS and Amazon RedShift, as a few examples.

Here I am in the AWS Console and first I need to create an EMR cluster to work with. To do that, I will navigate to the EMR dashboard by typing EMR into the search bar, then selecting EMR.

From here, I can create the cluster, and for this I want to launch a serverless cluster. So I will select “EMR serverless” from the navigation, then “get started”.

To manage EMR Serverless applications, you need EMR Studio, so I will select create and launch EMR Studio.

Now, we can create an application for EMR serverless. I’ll give the application the name example-app, leave the type as Spark, and accept the default release version.

Then I need to select the application setup options. I can choose from accepting the defaults for batch workloads, accepting the defaults for interactive workloads, or using custom settings. 

I am going to use the defaults for interactive workloads, which includes enabling the interactive endpoint for EMR studio, which needs to be configured so that we can use EMR studio to run applications on our cluster.

Now I will select create and start application.

The application is starting, which will take a few minutes. We’ll come back when this is done.

The application is now started, which means I can create the workspace and notebook that we will use to run spark jobs.

I’ll select Dashboard, then create workspace. This brings up an error at the top that tells me I need to enable this studio for interactive workspaces. So, I’ll select edit studio. This takes me to a page where I can edit the configurations for my studio.

To use Workspaces for interactive workloads, you have to configure the storage location for Workspaces and the studio service role. So I’ll select an existing IAM role for this studio, this IAM role let’s the studio interact with other AWS resources.

Then I will also navigate to an existing S3 storage location in this account. This is the backup location for workspaces. 

Once both of these values are filled out, I’ll select save changes and this studio has now been updated.

I can then select view workspaces to go back to creating the workspace needed to launch the notebook.

From here, I’ll select create workspace. Then I can give the workspace a name, I’ll call this example-workspace, then select create workspace, accepting the rest of the default values. 

Now that the workspace is created, I can launch it to gain access to the notebook.

Once I open the workspace, I can check to see if it’s connected to any specific EMR application because this is just a front-end to interact with the EMR cluster on the backend. We need to attach this notebook to compute resources.

So if I select the compute tab, I’ll see that this notebook is not connected to an EMR serverless application. To do that, I will select the dropdown to select the application we created earlier. Then, I also need to select the interactive runtime role the notebook will be using. This is the role that will be assumed to call other AWS services on your behalf. I’ll select the dropdown, and select emr-serverless-role, which is an IAM role already created in this account.  Now that this is configured, I can submit jobs to this application.

Now if I go back to the files in this workspace, there is a notebook created for me that is empty. So I’ll select that. Then I need to choose which kernel I want to use. I am going to select PySpark for the kernel.

Next I want to run a very simple PySpark script to do some data analysis on a file I have uploaded to an S3 bucket in my account. This file contains generated sample taxi data, showing information like the duration of the taxi ride, the fare for the taxi ride, and timestamps. I want to write a script that will calculate the average fare for the taxi rides between two dates.

So, I will paste in a script to perform that task, then hit shift-enter to run it. This will take some time to run and the output will show under the code snippet.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg

spark = SparkSession.builder.appName("AverageFareAmount").getOrCreate()

#Define S3 bucket path to the CSV file
s3_bucket_path = "s3a://sample-taxi-data-2024/fake_taxi_data.csv"

#Read the dataset from S3 into a Spark DataFrame
df = spark.read.csv(s3_bucket_path, header=True, inferSchema=True)

#Define the time period for filtering
start_date = "2020-06-01"
end_date = "2020-06-30"

#Filter the DataFrame for the specified time period
filtered_df = df.filter((col("pickup_datetime") >= start_date) & (col("pickup_datetime") <= end_date))

#Calculate the average fare amount
average_fare = filtered_df.select(avg(col("fare_amount")).alias("average_fare_amount"))

#Show the result
average_fare.show()

#Stop the Spark session
spark.stop()



Okay, so the job is done running and the average fare for the rides contained in this data set comes out to be 50 dollars and 64 cents.

This code was run as a job on the EMR serverless cluster this notebook is attached to, and behind the scenes, this code had to called some AWS APIs to be able to read the file from S3. 


When you’re working with PySpark, performing simple transformations, such as filtering, grouping, and aggregation, directly on a Spark DataFrame using Python, or coding these transformations as SQL queries generally results in comparable performance. This is because both Python and SQL approaches are ultimately translated into the same execution plan, and executed by the same underlying Spark computing engine. However, if the transformation is more complex, you might not be able to implement it in SQL, or at least not in a straightforward way. For instance, if you want to apply the transpose operation to swap the rows and columns of a table, you can simply call dot-T on the DataFrame. But transposing is not supported by Spark SQL. Or as another example, you can normalize columns and clean columns with missing values in SQL, but that might require more code than working with Python on Spark DataFrames. 

Moreover working with dataframes would help you write more testable, maintainable, and modular/reusable code. While reusable libraries are easy to create in Spark and PySpark, SQL doesn’t have a good notion of reusability for more complex query components. 

You also want to consider the skills and technical background of your team. You might find writing SQL queries to be simpler and easier than working with Python on Spark DataFrames. 

So depending on your transformation use case, you might find one of these approaches more suitable than the other. You can even try to combine both the Spark DataFrames and Spark SQL approaches to realize the best of both worlds.


Like you saw last week, instead of using Spark DataFrames, you could just use Pandas dataframe to process the data. However, Pandas is not a distributed framework; it will load the entire data into the memory of the machine on which your python code is running. So if your dataset is not very large, and by that I mean you can fit your entire data in memory, then you can use Pandas instead of Spark. In fact using Spark on a small dataset can be an overkill since you’ll need to manage the cluster of nodes. On the other hand, if your data is so large that it doesn’t fit entirely into memory or if you want to leverage distributed computations to enhance processing performance, then you should go with Spark and maybe run it on a cluster running on the cloud. 

In any case, whether you’re working on a single machine or cluster of nodes, the best practice would be to extract only the data you need from the source. The less data you have to process, the less resource-heavy and more performant your code will be. And so, you might need to apply transformations such as joining, grouping and filtering inside the database before ingesting the data to reduce the size of the data. 


Streaming transformations aim to prepare data for downstream consumption by converting a stream of events into another stream by enriching it with additional information or joining it with another stream. For instance, you might have an incoming stream carrying events from an IoT source. These IoT events carry a device ID and event data, and you might want to dynamically enrich these events with the device metadata stored in a separate database. You can use a stream-processing engine to query the separate database containing this metadata. Then you can generate a new stream of events by adding this metadata to the existing IoT events. In fact, you already have some experience with this. In one of the previous labs, you applied a streaming ETL to transform a stream of user session events by enriching each event with the timestamp representing the processing time and some additional metrics that you computed from the data of each user session. 

As another example of streaming transformation, you can use windowed queries to dynamically compute roll-up statistics on windows, and then send the output to a target stream. And in terms of joining two streams, you could for example use streaming transformation to combine a stream containing website clickstream data with another stream containing IoT data to get a unified view of the user activities.
In all of these examples, events are typically streamed to you by a streaming platform such as Kinesis data streams or Kafka, and then you can process these events using a stream processor. Similar to batch processing, there are distributed stream processing tools, like Spark streaming and Flink, that you can use when you have large data sets.  Both of these tools are open source and allow you to write python code or SQL queries to process large streams of data.

When choosing a streaming processing tool, it’s important to understand your use case, the latency requirements, and the performance capabilities of the framework in question. Some of these tools, like Spark Streaming, process your data in a micro-batch way providing near real-time performance. It accumulates small batches of input data, anywhere from 2 minutes to seconds, then processes each batch in parallel using a distributed collection of tasks, similar to the execution of a batch job in Spark. On the other hand, true streaming systems, such as Flink, are designed to process one event at a time. Each node in the system is continuously listening to messages from other nodes and outputting new updates to its dependent nodes. True streaming systems can deliver the processed events at a lower latency than the micro-batch processing systems. However, this comes with significant overhead. So depending on your use case and the acceptable latency, you may choose one over the other. If you are collecting sales metrics published every few minutes, micro-batches are probably just fine as long as you set an appropriate micro-batch frequency. On the other hand, if your ops team is computing metrics every milliseconds to detect malicious attacks, you might need true streaming.